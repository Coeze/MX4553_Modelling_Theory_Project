{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2b136ae",
   "metadata": {},
   "source": [
    "# Wildfire Detection and Segmentation using CNNs\n",
    "\n",
    "This notebook implements a Convolutional Neural Network (CNN) approach for wildfire detection and segmentation using satellite imagery. We'll utilize pre-fire and post-fire images from the MTBS dataset to train a model that can detect and segment burned areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab9c4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import seaborn as sns\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "import glob\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Check TensorFlow version and GPU availability\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append(os.path.join(os.getcwd(), 'src'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc2b462",
   "metadata": {},
   "source": [
    "## Load and Prepare MTBS Fire Data\n",
    "\n",
    "We'll load the pre-fire and post-fire satellite imagery along with burn perimeter information from the MTBS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bb635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find available fire data\n",
    "def find_fire_data():\n",
    "    data_dir = os.path.join(os.getcwd(), 'data')\n",
    "    fire_folders = [f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))]\n",
    "    return data_dir, fire_folders\n",
    "\n",
    "# Get available fire data\n",
    "data_dir, fire_folders = find_fire_data()\n",
    "print(f\"Found {len(fire_folders)} fire datasets:\")\n",
    "for folder in fire_folders:\n",
    "    print(f\"- {folder}\")\n",
    "\n",
    "# Function to load MTBS fire data for a specific fire\n",
    "def load_fire_data(fire_folder):\n",
    "    fire_path = os.path.join(data_dir, fire_folder)\n",
    "    \n",
    "    # Find pre-fire and post-fire reflectance data\n",
    "    refl_files = glob.glob(os.path.join(fire_path, f\"*_refl.tif\"))\n",
    "    \n",
    "    if len(refl_files) < 2:\n",
    "        print(f\"Insufficient reflectance files for {fire_folder}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Sort by date (assuming filename contains date)\n",
    "    refl_files.sort()\n",
    "    pre_fire_raster = refl_files[0]\n",
    "    post_fire_raster = refl_files[1]\n",
    "    \n",
    "    # Find burn perimeter shapefile\n",
    "    burn_bndy_files = glob.glob(os.path.join(fire_path, f\"*_burn_bndy.shp\"))\n",
    "    if not burn_bndy_files:\n",
    "        print(f\"No burn boundary shapefile found for {fire_folder}\")\n",
    "        return pre_fire_raster, post_fire_raster, None\n",
    "    \n",
    "    burn_bndy = burn_bndy_files[0]\n",
    "    \n",
    "    print(f\"Pre-fire: {os.path.basename(pre_fire_raster)}\")\n",
    "    print(f\"Post-fire: {os.path.basename(post_fire_raster)}\")\n",
    "    print(f\"Burn boundary: {os.path.basename(burn_bndy)}\")\n",
    "    \n",
    "    return pre_fire_raster, post_fire_raster, burn_bndy\n",
    "\n",
    "# Select the first fire dataset as an example\n",
    "if fire_folders:\n",
    "    selected_fire = fire_folders[0]\n",
    "    print(f\"\\nLoading data for fire: {selected_fire}\")\n",
    "    pre_fire_raster, post_fire_raster, burn_bndy = load_fire_data(selected_fire)\n",
    "else:\n",
    "    print(\"No fire datasets found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11afe8f",
   "metadata": {},
   "source": [
    "## Visualize the Fire Data\n",
    "\n",
    "Let's visualize the pre-fire and post-fire imagery along with the burn perimeter to understand the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9ca473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize reflectance data with burn perimeter overlay\n",
    "def visualize_fire_data(pre_fire_path, post_fire_path, burn_bndy_path):\n",
    "    if not (pre_fire_path and post_fire_path):\n",
    "        print(\"Missing pre-fire or post-fire data\")\n",
    "        return\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20, 7))\n",
    "    \n",
    "    # Load and display pre-fire image\n",
    "    with rasterio.open(pre_fire_path) as src:\n",
    "        pre_fire_data = src.read([4, 3, 2])  # NIR, Red, Green as RGB\n",
    "        pre_fire_data = np.transpose(pre_fire_data, (1, 2, 0))\n",
    "        # Normalize for visualization\n",
    "        pre_fire_data = pre_fire_data.astype(np.float32)\n",
    "        for i in range(3):\n",
    "            band = pre_fire_data[:,:,i]\n",
    "            min_val = np.percentile(band, 2)\n",
    "            max_val = np.percentile(band, 98)\n",
    "            pre_fire_data[:,:,i] = np.clip((band - min_val) / (max_val - min_val), 0, 1)\n",
    "        \n",
    "        axs[0].imshow(pre_fire_data)\n",
    "        axs[0].set_title(\"Pre-Fire Image (False Color)\")\n",
    "        axs[0].axis('off')\n",
    "        transform = src.transform\n",
    "        crs = src.crs\n",
    "    \n",
    "    # Load and display post-fire image\n",
    "    with rasterio.open(post_fire_path) as src:\n",
    "        post_fire_data = src.read([4, 3, 2])  # NIR, Red, Green as RGB\n",
    "        post_fire_data = np.transpose(post_fire_data, (1, 2, 0))\n",
    "        # Normalize for visualization\n",
    "        post_fire_data = post_fire_data.astype(np.float32)\n",
    "        for i in range(3):\n",
    "            band = post_fire_data[:,:,i]\n",
    "            min_val = np.percentile(band, 2)\n",
    "            max_val = np.percentile(band, 98)\n",
    "            post_fire_data[:,:,i] = np.clip((band - min_val) / (max_val - min_val), 0, 1)\n",
    "        \n",
    "        axs[1].imshow(post_fire_data)\n",
    "        axs[1].set_title(\"Post-Fire Image (False Color)\")\n",
    "        axs[1].axis('off')\n",
    "    \n",
    "    # Calculate NDVI change\n",
    "    with rasterio.open(pre_fire_path) as pre_src, rasterio.open(post_fire_path) as post_src:\n",
    "        pre_nir = pre_src.read(4).astype(np.float32)\n",
    "        pre_red = pre_src.read(3).astype(np.float32)\n",
    "        pre_ndvi = (pre_nir - pre_red) / (pre_nir + pre_red + 1e-6)  # Add small value to avoid division by zero\n",
    "        \n",
    "        post_nir = post_src.read(4).astype(np.float32)\n",
    "        post_red = post_src.read(3).astype(np.float32)\n",
    "        post_ndvi = (post_nir - post_red) / (post_nir + post_red + 1e-6)\n",
    "        \n",
    "        ndvi_diff = pre_ndvi - post_ndvi\n",
    "        \n",
    "        # Plot NDVI difference\n",
    "        im = axs[2].imshow(ndvi_diff, cmap='RdYlGn_r', vmin=-0.5, vmax=0.5)\n",
    "        axs[2].set_title(\"NDVI Change (Pre - Post)\")\n",
    "        axs[2].axis('off')\n",
    "        plt.colorbar(im, ax=axs[2], label='NDVI Difference')\n",
    "    \n",
    "    # If burn perimeter is available, overlay it on all images\n",
    "    if burn_bndy_path:\n",
    "        try:\n",
    "            burn_gdf = gpd.read_file(burn_bndy_path)\n",
    "            \n",
    "            # Transform to raster CRS if needed\n",
    "            if burn_gdf.crs != crs:\n",
    "                burn_gdf = burn_gdf.to_crs(crs)\n",
    "            \n",
    "            # For each subplot, overlay the burn perimeter\n",
    "            for ax in axs:\n",
    "                burn_gdf.boundary.plot(ax=ax, color='red', linewidth=2)\n",
    "                \n",
    "            print(f\"Burn perimeter overlaid successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error overlaying burn perimeter: {e}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return pre_fire_data, post_fire_data, ndvi_diff\n",
    "\n",
    "# Visualize the fire data\n",
    "if 'pre_fire_raster' in locals() and pre_fire_raster:\n",
    "    pre_fire_img, post_fire_img, ndvi_diff = visualize_fire_data(pre_fire_raster, post_fire_raster, burn_bndy)\n",
    "else:\n",
    "    print(\"No fire data available to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4551721",
   "metadata": {},
   "source": [
    "## Prepare Training Data for CNN\n",
    "\n",
    "Now we'll prepare the data for training our CNN model. We'll create a dataset of image patches with corresponding burned/unburned labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e60c136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create burned area mask from burn perimeter\n",
    "def create_burn_mask(burn_bndy_path, reference_raster_path):\n",
    "    # Open the reference raster to get dimensions and transform\n",
    "    with rasterio.open(reference_raster_path) as src:\n",
    "        height = src.height\n",
    "        width = src.width\n",
    "        transform = src.transform\n",
    "        crs = src.crs\n",
    "    \n",
    "    # Read the burn perimeter shapefile\n",
    "    burn_gdf = gpd.read_file(burn_bndy_path)\n",
    "    \n",
    "    # Transform to the same CRS as the raster if needed\n",
    "    if burn_gdf.crs != crs:\n",
    "        burn_gdf = burn_gdf.to_crs(crs)\n",
    "    \n",
    "    # Create a mask where 1=burned, 0=unburned\n",
    "    from rasterio.features import rasterize\n",
    "    shapes = [(geom, 1) for geom in burn_gdf.geometry]\n",
    "    burn_mask = rasterize(shapes, out_shape=(height, width), transform=transform, fill=0, dtype=np.uint8)\n",
    "    \n",
    "    return burn_mask\n",
    "\n",
    "# Function to prepare training data\n",
    "def prepare_training_data(pre_fire_path, post_fire_path, burn_bndy_path, patch_size=64, stride=32):\n",
    "    print(\"Preparing training data...\")\n",
    "    \n",
    "    # Create burn mask\n",
    "    burn_mask = create_burn_mask(burn_bndy_path, post_fire_path)\n",
    "    print(f\"Created burn mask with shape {burn_mask.shape}\")\n",
    "    \n",
    "    # Load pre-fire and post-fire images\n",
    "    with rasterio.open(pre_fire_path) as pre_src, rasterio.open(post_fire_path) as post_src:\n",
    "        # Read visible + NIR bands\n",
    "        pre_bands = pre_src.read([1, 2, 3, 4])  # Blue, Green, Red, NIR\n",
    "        post_bands = post_src.read([1, 2, 3, 4])\n",
    "        \n",
    "        # Transpose to height, width, channels\n",
    "        pre_img = np.transpose(pre_bands, (1, 2, 0))\n",
    "        post_img = np.transpose(post_bands, (1, 2, 0))\n",
    "        \n",
    "        # Calculate NDVI for pre and post\n",
    "        pre_ndvi = (pre_bands[3] - pre_bands[2]) / (pre_bands[3] + pre_bands[2] + 1e-6)\n",
    "        post_ndvi = (post_bands[3] - post_bands[2]) / (post_bands[3] + post_bands[2] + 1e-6)\n",
    "        ndvi_diff = pre_ndvi - post_ndvi\n",
    "        \n",
    "        # Stack all features: pre-image (4 bands), post-image (4 bands), NDVI difference (1 band)\n",
    "        X = np.zeros((pre_img.shape[0], pre_img.shape[1], 9), dtype=np.float32)\n",
    "        X[:,:,0:4] = pre_img / 10000.0  # Normalize reflectance values\n",
    "        X[:,:,4:8] = post_img / 10000.0\n",
    "        X[:,:,8] = ndvi_diff\n",
    "        \n",
    "        # Target is burn mask\n",
    "        y = burn_mask\n",
    "        \n",
    "        # Create patches\n",
    "        patches_X = []\n",
    "        patches_y = []\n",
    "        \n",
    "        h, w = X.shape[:2]\n",
    "        for i in range(0, h - patch_size + 1, stride):\n",
    "            for j in range(0, w - patch_size + 1, stride):\n",
    "                patch_X = X[i:i+patch_size, j:j+patch_size]\n",
    "                patch_y = y[i:i+patch_size, j:j+patch_size]\n",
    "                \n",
    "                # Only include patches that have some burned or unburned pixels\n",
    "                if np.any(patch_y) and np.mean(patch_y) < 0.9:\n",
    "                    patches_X.append(patch_X)\n",
    "                    patches_y.append(patch_y)\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        patches_X = np.array(patches_X)\n",
    "        patches_y = np.array(patches_y)\n",
    "        \n",
    "        print(f\"Created {len(patches_X)} patches with shape {patches_X.shape}\")\n",
    "        \n",
    "        return patches_X, patches_y\n",
    "\n",
    "# Prepare training data if fire data is available\n",
    "if 'pre_fire_raster' in locals() and pre_fire_raster and burn_bndy:\n",
    "    try:\n",
    "        X_patches, y_patches = prepare_training_data(pre_fire_raster, post_fire_raster, burn_bndy, patch_size=64, stride=32)\n",
    "        \n",
    "        # Split into training and validation sets\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_patches, y_patches, test_size=0.2, random_state=42)\n",
    "        \n",
    "        print(f\"Training data: {X_train.shape}, {y_train.shape}\")\n",
    "        print(f\"Validation data: {X_val.shape}, {y_val.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error preparing training data: {e}\")\n",
    "else:\n",
    "    print(\"Cannot prepare training data: Missing fire data or burn boundary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27750c9c",
   "metadata": {},
   "source": [
    "## Create and Train the CNN Model\n",
    "\n",
    "We'll create a U-Net style CNN for semantic segmentation of burned areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bc3bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define U-Net model for semantic segmentation\n",
    "def create_unet_model(input_shape):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder (downsampling)\n",
    "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
    "    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    conv3 = layers.Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = layers.Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
    "    pool3 = layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    # Bridge\n",
    "    conv4 = layers.Conv2D(512, 3, activation='relu', padding='same')(pool3)\n",
    "    conv4 = layers.Conv2D(512, 3, activation='relu', padding='same')(conv4)\n",
    "    \n",
    "    # Decoder (upsampling)\n",
    "    up5 = layers.Conv2DTranspose(256, 2, strides=(2, 2), padding='same')(conv4)\n",
    "    up5 = layers.concatenate([up5, conv3])\n",
    "    conv5 = layers.Conv2D(256, 3, activation='relu', padding='same')(up5)\n",
    "    conv5 = layers.Conv2D(256, 3, activation='relu', padding='same')(conv5)\n",
    "    \n",
    "    up6 = layers.Conv2DTranspose(128, 2, strides=(2, 2), padding='same')(conv5)\n",
    "    up6 = layers.concatenate([up6, conv2])\n",
    "    conv6 = layers.Conv2D(128, 3, activation='relu', padding='same')(up6)\n",
    "    conv6 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv6)\n",
    "    \n",
    "    up7 = layers.Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(conv6)\n",
    "    up7 = layers.concatenate([up7, conv1])\n",
    "    conv7 = layers.Conv2D(64, 3, activation='relu', padding='same')(up7)\n",
    "    conv7 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv7)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = layers.Conv2D(1, 1, activation='sigmoid')(conv7)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Create and compile model if training data is available\n",
    "if 'X_train' in locals() and len(X_train) > 0:\n",
    "    try:\n",
    "        # Create model\n",
    "        input_shape = X_train.shape[1:]\n",
    "        model = create_unet_model(input_shape)\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy', tf.keras.metrics.Recall(), tf.keras.metrics.Precision()]\n",
    "        )\n",
    "        \n",
    "        # Model summary\n",
    "        model.summary()\n",
    "        \n",
    "        # Define callbacks\n",
    "        callbacks = [\n",
    "            EarlyStopping(patience=5, restore_best_weights=True),\n",
    "            ModelCheckpoint('best_fire_cnn_model.h5', save_best_only=True)\n",
    "        ]\n",
    "        \n",
    "        # Train model\n",
    "        print(\"Training CNN model...\")\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=30,\n",
    "            batch_size=16,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        \n",
    "        # Plot training history\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['loss'], label='Train')\n",
    "        plt.plot(history.history['val_loss'], label='Validation')\n",
    "        plt.title('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Binary Crossentropy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['accuracy'], label='Train')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "        plt.title('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating or training model: {e}\")\n",
    "else:\n",
    "    print(\"Cannot train model: No training data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd28bc8f",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance\n",
    "\n",
    "Let's evaluate the model's performance on the validation set and visualize some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3985906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model if available\n",
    "if 'model' in locals() and 'X_val' in locals():\n",
    "    # Evaluate on validation set\n",
    "    val_loss, val_acc, val_recall, val_precision = model.evaluate(X_val, y_val)\n",
    "    val_f1 = 2 * (val_precision * val_recall) / (val_precision + val_recall)\n",
    "    \n",
    "    print(\"\\nValidation Performance:\")\n",
    "    print(f\"Loss: {val_loss:.4f}\")\n",
    "    print(f\"Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"Precision: {val_precision:.4f}\")\n",
    "    print(f\"Recall: {val_recall:.4f}\")\n",
    "    print(f\"F1 Score: {val_f1:.4f}\")\n",
    "    \n",
    "    # Make predictions on validation set\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred_binary = (y_pred > 0.5).astype(np.uint8)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    y_true_flat = y_val.flatten()\n",
    "    y_pred_flat = y_pred_binary.flatten()\n",
    "    cm = confusion_matrix(y_true_flat, y_pred_flat)\n",
    "    \n",
    "    # Visualize confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "                xticklabels=['Unburned', 'Burned'],\n",
    "                yticklabels=['Unburned', 'Burned'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize some predictions\n",
    "    n_samples = min(5, len(X_val))\n",
    "    plt.figure(figsize=(15, n_samples * 3))\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Display pre-fire image (false color: NIR, Red, Green)\n",
    "        plt.subplot(n_samples, 4, i*4 + 1)\n",
    "        rgb = np.zeros((X_val.shape[1], X_val.shape[2], 3))\n",
    "        rgb[:,:,0] = X_val[i,:,:,3]  # NIR\n",
    "        rgb[:,:,1] = X_val[i,:,:,2]  # Red\n",
    "        rgb[:,:,2] = X_val[i,:,:,1]  # Green\n",
    "        # Normalize for visualization\n",
    "        for j in range(3):\n",
    "            min_val = np.percentile(rgb[:,:,j], 2)\n",
    "            max_val = np.percentile(rgb[:,:,j], 98)\n",
    "            rgb[:,:,j] = np.clip((rgb[:,:,j] - min_val) / (max_val - min_val), 0, 1)\n",
    "        plt.imshow(rgb)\n",
    "        plt.title(f\"Pre-Fire (Sample {i+1})\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Display post-fire image\n",
    "        plt.subplot(n_samples, 4, i*4 + 2)\n",
    "        rgb = np.zeros((X_val.shape[1], X_val.shape[2], 3))\n",
    "        rgb[:,:,0] = X_val[i,:,:,7]  # NIR\n",
    "        rgb[:,:,1] = X_val[i,:,:,6]  # Red\n",
    "        rgb[:,:,2] = X_val[i,:,:,5]  # Green\n",
    "        # Normalize for visualization\n",
    "        for j in range(3):\n",
    "            min_val = np.percentile(rgb[:,:,j], 2)\n",
    "            max_val = np.percentile(rgb[:,:,j], 98)\n",
    "            rgb[:,:,j] = np.clip((rgb[:,:,j] - min_val) / (max_val - min_val), 0, 1)\n",
    "        plt.imshow(rgb)\n",
    "        plt.title(f\"Post-Fire (Sample {i+1})\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Display true burn mask\n",
    "        plt.subplot(n_samples, 4, i*4 + 3)\n",
    "        plt.imshow(y_val[i,:,:,0], cmap='gray')\n",
    "        plt.title(f\"True Burn Mask\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Display predicted burn mask\n",
    "        plt.subplot(n_samples, 4, i*4 + 4)\n",
    "        plt.imshow(y_pred[i,:,:,0], cmap='gray')\n",
    "        plt.title(f\"Predicted Burn Mask\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No model or validation data available for evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6bd20b",
   "metadata": {},
   "source": [
    "## Apply the Model to a New Fire\n",
    "\n",
    "Let's test our model on a different fire dataset to assess its generalization capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521b6264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply trained model to a new fire dataset\n",
    "def apply_model_to_new_fire(model, fire_folder):\n",
    "    print(f\"Applying model to new fire dataset: {fire_folder}\")\n",
    "    \n",
    "    # Load fire data\n",
    "    pre_fire_raster, post_fire_raster, burn_bndy = load_fire_data(fire_folder)\n",
    "    \n",
    "    if not (pre_fire_raster and post_fire_raster):\n",
    "        print(\"Cannot apply model: Missing fire data\")\n",
    "        return\n",
    "    \n",
    "    # Load the raster data\n",
    "    with rasterio.open(pre_fire_raster) as pre_src, rasterio.open(post_fire_raster) as post_src:\n",
    "        # Get metadata for output\n",
    "        out_meta = post_src.meta.copy()\n",
    "        \n",
    "        # Read visible + NIR bands\n",
    "        pre_bands = pre_src.read([1, 2, 3, 4])  # Blue, Green, Red, NIR\n",
    "        post_bands = post_src.read([1, 2, 3, 4])\n",
    "        \n",
    "        # Transpose to height, width, channels\n",
    "        pre_img = np.transpose(pre_bands, (1, 2, 0))\n",
    "        post_img = np.transpose(post_bands, (1, 2, 0))\n",
    "        \n",
    "        # Calculate NDVI for pre and post\n",
    "        pre_ndvi = (pre_bands[3] - pre_bands[2]) / (pre_bands[3] + pre_bands[2] + 1e-6)\n",
    "        post_ndvi = (post_bands[3] - post_bands[2]) / (post_bands[3] + post_bands[2] + 1e-6)\n",
    "        ndvi_diff = pre_ndvi - post_ndvi\n",
    "        \n",
    "        # Load actual burn mask if available\n",
    "        if burn_bndy:\n",
    "            actual_mask = create_burn_mask(burn_bndy, post_fire_raster)\n",
    "        else:\n",
    "            actual_mask = None\n",
    "        \n",
    "        # Prepare input data\n",
    "        height, width = pre_img.shape[:2]\n",
    "        X = np.zeros((height, width, 9), dtype=np.float32)\n",
    "        X[:,:,0:4] = pre_img / 10000.0  # Normalize reflectance values\n",
    "        X[:,:,4:8] = post_img / 10000.0\n",
    "        X[:,:,8] = ndvi_diff\n",
    "        \n",
    "        # Process in patches to avoid memory issues\n",
    "        patch_size = 64\n",
    "        stride = 32  # For overlapping predictions\n",
    "        \n",
    "        # Create an empty prediction mask\n",
    "        pred_mask = np.zeros((height, width), dtype=np.float32)\n",
    "        count_mask = np.zeros((height, width), dtype=np.float32)  # For overlapping normalization\n",
    "        \n",
    "        # Generate predictions for each patch\n",
    "        for i in range(0, height - patch_size + 1, stride):\n",
    "            for j in range(0, width - patch_size + 1, stride):\n",
    "                patch = X[i:i+patch_size, j:j+patch_size]\n",
    "                \n",
    "                # Add batch dimension\n",
    "                patch = np.expand_dims(patch, axis=0)\n",
    "                \n",
    "                # Predict\n",
    "                pred = model.predict(patch)[0,:,:,0]\n",
    "                \n",
    "                # Accumulate predictions (for overlapping areas)\n",
    "                pred_mask[i:i+patch_size, j:j+patch_size] += pred\n",
    "                count_mask[i:i+patch_size, j:j+patch_size] += 1\n",
    "        \n",
    "        # Average overlapping predictions\n",
    "        pred_mask = pred_mask / np.maximum(count_mask, 1)  # Avoid division by zero\n",
    "        \n",
    "        # Create binary mask\n",
    "        binary_mask = (pred_mask > 0.5).astype(np.uint8)\n",
    "        \n",
    "        # Display results\n",
    "        fig, axs = plt.subplots(1, 3 if actual_mask is not None else 2, figsize=(15, 5))\n",
    "        \n",
    "        # NDVI difference\n",
    "        im = axs[0].imshow(ndvi_diff, cmap='RdYlGn_r', vmin=-0.5, vmax=0.5)\n",
    "        axs[0].set_title(\"NDVI Change (Pre - Post)\")\n",
    "        axs[0].axis('off')\n",
    "        plt.colorbar(im, ax=axs[0], label='NDVI Difference')\n",
    "        \n",
    "        # Model prediction\n",
    "        axs[1].imshow(binary_mask, cmap='gray')\n",
    "        axs[1].set_title(\"Model Prediction (Burned Areas)\")\n",
    "        axs[1].axis('off')\n",
    "        \n",
    "        # Actual burn mask (if available)\n",
    "        if actual_mask is not None:\n",
    "            axs[2].imshow(actual_mask, cmap='gray')\n",
    "            axs[2].set_title(\"Actual Burned Areas\")\n",
    "            axs[2].axis('off')\n",
    "            \n",
    "            # Calculate accuracy metrics\n",
    "            acc = accuracy_score(actual_mask.flatten(), binary_mask.flatten())\n",
    "            prec = precision_score(actual_mask.flatten(), binary_mask.flatten())\n",
    "            rec = recall_score(actual_mask.flatten(), binary_mask.flatten())\n",
    "            f1 = f1_score(actual_mask.flatten(), binary_mask.flatten())\n",
    "            \n",
    "            print(f\"\\nModel Performance on New Fire:\")\n",
    "            print(f\"Accuracy: {acc:.4f}\")\n",
    "            print(f\"Precision: {prec:.4f}\")\n",
    "            print(f\"Recall: {rec:.4f}\")\n",
    "            print(f\"F1 Score: {f1:.4f}\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Save prediction as GeoTIFF\n",
    "        out_meta.update({\n",
    "            'dtype': 'uint8',\n",
    "            'count': 1\n",
    "        })\n",
    "        \n",
    "        output_file = os.path.join(os.getcwd(), f\"{fire_folder}_cnn_prediction.tif\")\n",
    "        with rasterio.open(output_file, 'w', **out_meta) as dest:\n",
    "            dest.write(binary_mask.astype(np.uint8), 1)\n",
    "        \n",
    "        print(f\"Prediction saved to: {output_file}\")\n",
    "\n",
    "# If we have a trained model and multiple fire datasets, test on another one\n",
    "if 'model' in locals() and len(fire_folders) > 1:\n",
    "    try:\n",
    "        # Use the second fire dataset for testing\n",
    "        test_fire = fire_folders[1]\n",
    "        apply_model_to_new_fire(model, test_fire)\n",
    "    except Exception as e:\n",
    "        print(f\"Error applying model to new fire: {e}\")\n",
    "else:\n",
    "    print(\"Cannot test model on new fire: Missing model or additional fire datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38712ce",
   "metadata": {},
   "source": [
    "## Conclusion and Future Work\n",
    "\n",
    "In this notebook, we've built a CNN-based approach for wildfire burn area detection and segmentation. The model leverages multi-spectral satellite imagery and can effectively identify burned areas from pre-fire and post-fire imagery.\n",
    "\n",
    "Future improvements could include:\n",
    "\n",
    "1. **More training data**: Incorporating more diverse fire events for better generalization.\n",
    "2. **Time series analysis**: Including multiple time points to capture the progression of fire and recovery.\n",
    "3. **Additional features**: Incorporating topographic data (slope, aspect, elevation) and weather conditions.\n",
    "4. **Model refinement**: Experimenting with different architectures like DeepLabV3+ or attention mechanisms.\n",
    "5. **Early detection**: Adapting the model for early fire detection rather than post-fire mapping.\n",
    "6. **Severity classification**: Extending the model to classify burn severity levels rather than binary classification.\n",
    "\n",
    "The trained model could be integrated with the cellular automata simulation system to provide better initialization or validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b292947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model for later use\n",
    "if 'model' in locals():\n",
    "    model_path = 'fire_detection_cnn_model.h5'\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    # Also save as TensorFlow SavedModel format for deployment\n",
    "    tf_model_path = 'fire_detection_model'\n",
    "    tf.saved_model.save(model, tf_model_path)\n",
    "    print(f\"Model saved in TensorFlow SavedModel format to {tf_model_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
